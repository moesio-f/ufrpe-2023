- Contexto Histórico
	- Surge como uma inspiração biológica do cérebro Humano
		- Mas antes, faz-se necessário conhecê-lo
		- Descoberta do Microscópio (~1590)
		- Descoberta da Célula (~1680)
		- Célula como unidade constituinte dos seres vivos (~1830)
		- Constituintes básicos do cérebro são neurônios (~1909)
	- Cérebro Humano
		- $10^{11}$ neurônios e $10^{14}$ sinapses/conexões
		- Apresenta em média $1000$ conexões por neurônio, podendo chegar até $10000$
		- Em seres humanos, 70% dos neurônios se encontram no córtex
		- O cérebro consome 20% da energia do corpo
	- Neurônio Biológico
		- ![Neuron - Wikipedia](https://upload.wikimedia.org/wikipedia/commons/1/10/Blausen_0657_MultipolarNeuron.png)
		- Existem múltiplos tipos de neurônios, responsáveis por tarefas específicas
			- ![How many types of neurons do we need to define?](https://scx2.b-cdn.net/gfx/news/2013/howmanytypes.jpg)
	- Redes Neurais Artificiais
		- McCulloch & Putts (1943)
			- Modelagem de um neurônio artificial com inspiração no cérebro humano
			- O neurônio tem um comportamento binário (ativo ou não ativo)
				- Seja $s(\vec{x}_t) = \sum_i x_{t,i} \cdot w_{t,i}$ a soma ponderada da entrada $\vec{x}_t$ para o neurônio artificial com pesos $\vec{w}_t$
					- Para um dado tempo $t$, seja $u_t = s(\vec{x}_t)$
				- Seja $h: \mathbb{R} \to \{0, 1\}$ uma função de ativação baseada em threshold com limiar $\theta \in \mathbb{R}$
					- Se $u_t \geq \theta$, $h(u_t) = 1$
					- Do contrário, $h(u_t) = 0$
			- Apenas com esse modelo simples já conseguimos implementar o funcionamento das portas lógicas AND, OR e NOT
		- Perceptron de Rosenblatt
			- Propôs uma nova modelagem do neurônio artificial, adicionando um *bias* $b_k$ e com maior relaxamento na função de ativação $\varphi$, e um algoritmo para busca dos pesos (e viés)
			- ![image.png](../assets/image_1696622340638_0.png){:height 285, :width 688}
		- ADALINE
		- Resolução do problema XOR
		- *Dark Age* -> baixa em IA
		- Perceptron Multi-camadas (MLP + Backpropagation)
		- Support Vector Machine (SVM)
		- Deep Neural Network (DNN)
		- ![image.png](../assets/image_1696622683331_0.png)
- Como funciona uma Rede Neural na prática?
	- Exemplo em um problema de classificação de aceitação ou recusa de um estudante em uma Universidade
		- Podemos modelar como um problema de *otimização* para buscar a reta de separação entre os dados e depois utilizar esse limiar para seleção da classe
		- Essencialmente, podemos modelar esse problema utilizando o neurônio artificial simples de McCullho & Pitts
		- No final das contas, teríamos que buscar $\vec{w} = [w_0, w_1, w_2, \dots, w_n]$ de forma que a reta $\vec{w}^T \cdot \vec{x} = 0$, onde $\vec{x} = [1, x_1, x_2, \dots, x_n]$, **melhor** separasse os aprovados dos não aprovados
			- O **melhor** nesse caso pode ser definido matematicamente de diferentes formas
			- Por exemplo, podemos utilizar o caso da *regressão linear*
			- Também podemos utilizar diferentes outras *métricas*
- Atualização do Perceptron
  collapsed:: true
	- Seja $f: \mathbb{R} \to \{0, 1\}$ a função de ativação degrau
	- Seja $\vec{w} = [w_0, w_1, w_2] \in \mathbb{R}^3$ os pesos de um Perceptron
	- Seja $\vec{x} = [1, x_1, x_2] \in \mathbb{R}^3$ um ponto qualquer
	- Seja $u = \vec{w}^T\vec{x}$
	- Inicialmente, $f(u) = 0$
		- Isso implica que $w_0 + w_1x_1 + w_2x_2 < 0 \iff \vec{w}^T\vec{x} < 0$
	- Queremos atualizar $\vec{w}$ de forma que $f(u)$ se torne $1$
		- Ou seja, queremos que $\vec{w}^T\vec{x} \geq 0$
		- Dessa forma, queremos encontrar $\Delta\vec{w}$ tal que $(\Delta\vec{w}+\vec{w})^T\vec{x} \geq 0$
			- Qual um exemplo de $\Delta\vec{w}$ que resolve esse problema?
			- $\Delta w_0 + w_0 + (\Delta w_1 + w_1)x_1 + (\Delta w_2 + w_2)x_2$
		- Como podemos fazer isso?
			- Seja $K > 0 \in \mathbb{R}$
			- Assim, $\vec{w}^T\vec{x} + K < 0 + K$
- Como encontrar os coeficientes de uma linha $w_0 + w_1x_1 + w_2x_2 = 0$ que intercepte o ponto $[1, x_1, x_2]$?
	- A resposta é que existem infinitas linhas que satisfazem essa equação!
	- Podemos escolher quaisquer $w_{\{1, 2\}} \in \mathbb{R}$ e conseguimos selecionar o último componente como $w_0 = - (w_1x_1 + w_2x_2)$
	- Dessa forma, podemos utilizar qualquer estratégia para encontrar esses valores
	- Agora, como podemos encontrar os valores de $w_i$ de forma que a equação original seja $> 0$?
		- Estamos resolvendo essencialmente o mesmo problema, basta escolhermos $w_{\{1, 2\}} \in \mathbb{R}$ arbitrários e depois selecionar um $w_0 > -(w_1x_1 + w_2x_2)$
		- Por exemplo, podemos fazer $w_0 = -(w_1x_1 + w_2x_2) + \eta$ com $\eta > 0 \in \mathbb{R}$
	- E para ela ser $< 0$?
		- A mesma ideia só que $\eta < 0$
- Obtendo os pesos do Perceptron de Rosenblatt para um único ponto
	- Dependendo se a classificação para esse ponto é $0$ ou $1$ caímos em algum dos dois casos considerados
	- Sem perda de generalidade, vamos supor que esse ponto $\vec{x}$ deve ter classe positiva
		- Para calcular a classe de $\vec{x}$ com os pesos $\vec{w}$ fazemos $H(\vec{w}^T\vec{x})$ (Heaviside)
	- Vamos supor que começamos com um $\vec{w}$ qualquer que ainda não resolve o problema
	- Como podemos atualizar iterativamente os valores de $\vec{w}$ de forma que é garantido que encontramos o valor correto?
		- Qual são os valores de $w$ que resolvem o problema com certeza?
			- LATER
-