- Contexto Histórico
	- Surge como uma inspiração biológica do cérebro Humano
		- Mas antes, faz-se necessário conhecê-lo
		- Descoberta do Microscópio (~1590)
		- Descoberta da Célula (~1680)
		- Célula como unidade constituinte dos seres vivos (~1830)
		- Constituintes básicos do cérebro são neurônios (~1909)
	- Cérebro Humano
		- $10^{11}$ neurônios e $10^{14}$ sinapses/conexões
		- Apresenta em média $1000$ conexões por neurônio, podendo chegar até $10000$
		- Em seres humanos, 70% dos neurônios se encontram no córtex
		- O cérebro consome 20% da energia do corpo
	- Neurônio Biológico
		- ![Neuron - Wikipedia](https://upload.wikimedia.org/wikipedia/commons/1/10/Blausen_0657_MultipolarNeuron.png)
		- Existem múltiplos tipos de neurônios, responsáveis por tarefas específicas
			- ![How many types of neurons do we need to define?](https://scx2.b-cdn.net/gfx/news/2013/howmanytypes.jpg)
	- Redes Neurais Artificiais
		- McCulloch & Putts (1943)
			- Modelagem de um neurônio artificial com inspiração no cérebro humano
			- O neurônio tem um comportamento binário (ativo ou não ativo)
				- Seja $s(\vec{x}_t) = \sum_i x_{t,i} \cdot w_{t,i}$ a soma ponderada da entrada $\vec{x}_t$ para o neurônio artificial com pesos $\vec{w}_t$
					- Para um dado tempo $t$, seja $u_t = s(\vec{x}_t)$
				- Seja $h: \mathbb{R} \to \{0, 1\}$ uma função de ativação baseada em threshold com limiar $\theta \in \mathbb{R}$
					- Se $u_t \geq \theta$, $h(u_t) = 1$
					- Do contrário, $h(u_t) = 0$
			- Apenas com esse modelo simples já conseguimos implementar o funcionamento das portas lógicas AND, OR e NOT
		- Perceptron de Rosenblatt
			- Propôs uma nova modelagem do neurônio artificial, adicionando um *bias* $b_k$ e com maior relaxamento na função de ativação $\varphi$, e um algoritmo para busca dos pesos (e viés)
			- ![image.png](../assets/image_1696622340638_0.png){:height 285, :width 688}
		- ADALINE
		- Resolução do problema XOR
		- *Dark Age* -> baixa em IA
		- Perceptron Multi-camadas (MLP + Backpropagation)
		- Support Vector Machine (SVM)
		- Deep Neural Network (DNN)
		- ![image.png](../assets/image_1696622683331_0.png)
- Como funciona uma Rede Neural na prática?
	- Exemplo em um problema de classificação de aceitação ou recusa de um estudante em uma Universidade
		- Podemos modelar como um problema de *otimização* para buscar a reta de separação entre os dados e depois utilizar esse limiar para seleção da classe
		- Essencialmente, podemos modelar esse problema utilizando o neurônio artificial simples de McCullho & Pitts
		- No final das contas, teríamos que buscar $\vec{w} = [w_0, w_1, w_2, \dots, w_n]$ de forma que a reta $\vec{w}^T \cdot \vec{x} = 0$, onde $\vec{x} = [1, x_1, x_2, \dots, x_n]$, **melhor** separasse os aprovados dos não aprovados
			- O **melhor** nesse caso pode ser definido matematicamente de diferentes formas
			- Por exemplo, podemos utilizar o caso da *regressão linear*
			- Também podemos utilizar diferentes outras *métricas*
- Como encontrar os coeficientes de uma linha $w_0 + w_1x_1 + w_2x_2 = 0$ que intercepte o ponto $[1, x_1, x_2]$?
	- A resposta é que existem infinitas linhas que satisfazem essa equação!
	- Podemos escolher quaisquer $w_{\{1, 2\}} \in \mathbb{R}$ e conseguimos selecionar o último componente como $w_0 = - (w_1x_1 + w_2x_2)$
	- Dessa forma, podemos utilizar qualquer estratégia para encontrar esses valores
	- Agora, como podemos encontrar os valores de $w_i$ de forma que a equação original seja $> 0$?
		- Estamos resolvendo essencialmente o mesmo problema, basta escolhermos $w_{\{1, 2\}} \in \mathbb{R}$ arbitrários e depois selecionar um $w_0 > -(w_1x_1 + w_2x_2)$
		- Por exemplo, podemos fazer $w_0 = -(w_1x_1 + w_2x_2) + \eta$ com $\eta > 0 \in \mathbb{R}$
	- E para ela ser $< 0$?
		- A mesma ideia só que $\eta < 0$
- Obtendo os pesos do Perceptron para um único ponto
	- Dependendo se o rótulo para esse ponto é $0$ ou $1$, temos dois casos
	- Sem perda de generalidade, vamos supor que esse ponto $\vec{x} = [1, x_1, x_2]$ deve ter classe positiva
		- Para calcular a classe de $\vec{x}$ com os pesos $\vec{w}$ fazemos $H(\vec{w}^T\vec{x})$ (Heaviside)
		- $H(\vec{w}^T\vec{x}) = 1 \iff \vec{w}^T\vec{x} \geq 0$
	- Vamos supor que começamos com um $\vec{w}$ qualquer que **ainda não resolve o problema**
	- Como podemos atualizar iterativamente os valores de $\vec{w}$ de forma que é garantido que encontramos o valor correto?
		- Primeiro, perceba que $w_0 + w_1x_1 + w_2x_2 < 0$
		- Se $x_1 > 0$ e $x_2 > 0$, então
			- Podemos atualizar $\vec{w}$ de forma proporcional a $\vec{x}$ (i.e, $\vec{w}_{t+1} = \vec{w}_t + \eta \vec{x}$)
			- Tomando um valor $\eta > 0 \in \mathbb{R}$ grande o suficiente, $w_{i, t+1} > w_{i, t}$ e $w_{i, t+1} > 0$
			- No final das contas, $\vec{w}^T\vec{x} \geq 0$
		- Se $x_1 < 0$ e $x_2 < 0$, então
			- Equivalente ao caso anterior, entretanto os novos valores de $\vec{w}$ serão menores que os anteriores e $< 0$
		- Se $x_1 < 0$ e $x_2 > 0$, então
			- Equivalente aos dois casos anteriores, todavia uma das componentes será positiva e a outra negativa
		- Visto que nos 3 casos conseguimos realizar atualizações usando $\vec{x}$, podemos definir uma regra de atualização global no seguinte formato: $\vec{w}_{t+1} = \vec{w}_{t} + \eta \vec{x}$, com $\eta \in \mathbb{R}$
		- Para uma atualização iterativa, podemos usar a regra definida com um valor $0 < \eta \leq 1$ e com a condição de parada $w_0 + w_1x_1 + w_2x_2 \geq 0$
	- No final de todo o processo, a regra de atualização busca tornar $w_ix_i$ com o sinal correto para satisfazer a classificação correta
	- Uma prova de convergência pode ser definida utilizando essa regra de atualização e múltiplos pontos $\vec{x}_i$ com classes positivas e negativas